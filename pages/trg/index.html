<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <title>TensorNetwork</title>
    <meta http-equiv="content-language" content="en" />
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    <!--<link rel="stylesheet" href="https://tensornetwork.github.io/style.css" type="text/css"/>-->
    <link rel="stylesheet" href="/style.css" type="text/css"/>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.6.2/jquery.min.js" type="text/javascript"></script>

    <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['\$','\$'] ], displayMath: [ ['@@','@@'] ] }});
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_HTML">
    </script>
</head>

<body>

<div id="main">

<br/>
<div class="top_navbar">
    <div class="top_navbar_left">
        <img class="top_navbar_height" src="/tn_logo.png"/>
    </div>
    <div class="top_navbar_links top_navbar_middle"> 
        <a href="index.html">Main</a> <a href="about.html">About</a>
    </div>
    <div class="top_navbar_right"></div>
</div> <!--top_navbar-->
<span id="nav">
</span>

<div class="full">
<p>
</p>
</div>

<div class="full edit docs">
<h1>Case Study: TRG Algorithm</h1>
<p>The handful of techniques we have covered so far &#40;ITensor contraction and SVD&#41;
are already enough to implement a powerful algorithm: the <em>tensor renormalization group</em>
&#40;TRG&#41;.</p>
<p>First proposed by Levin and Nave &#40;cond-mat/0611687&#41;, TRG is a strategy for contracting a network
of tensors connected in a two-dimensional lattice pattern by decimating the network
in a heirarchical fashion. The term <a href="http://physics.ohio-state.edu/~jay/846/Wilson.pdf">&quot;renormalization group&quot;</a> 
refers to processes where less important information at small distance scales is 
repeatedly discarded until only the most important information remains.</p>
<h2>The Problem</h2>
<p>TRG can be used to compute certain large, non-trivial sums by exploiting
the fact that they can be recast as the contraction of a lattice of small tensors.</p>
<p>A classic example of such a sum is the &quot;partition function&quot; &#36;Z&#36; of the classical Ising
model at temperature T, defined to be</p>
<p>@@
Z &#61; \sum_&#123;\sigma_1 \sigma_2 \sigma_3 \ldots&#125; e^&#123;-E&#40;\sigma_1,\sigma_2,\sigma_3,\ldots&#41;/T&#125;
@@</p>
<p>where each Ising &quot;spin&quot; &#36;\sigma&#36; is just a variable taking the values &#36;\sigma &#61; &#43;1, -1&#36; and the energy
&#36;E&#40;\sigma_1,\sigma_2,\sigma_3,\ldots&#41;&#36; is the sum of products &#36;\sigma_i \sigma_j&#36; of 
neighboring &#36;\sigma&#36; variables.
In the two-dimensional case described below, there is a &quot;critical&quot; temperature &#36;T_c&#61;2.269\ldots&#36;
at which this Ising system develops an interesting hidden scale invariance.</p>
<h3>One dimension</h3>
<p>In one dimension, spins only have two neighbors since they are arranged along a chain.
For a finite-size system of N Ising spins, the usual convention is to use periodic boundary conditions 
meaning that the Nth spin connects back to the first around a circle:
@@
E&#40;\sigma_1,\sigma_2,\sigma_3,\ldots,\sigma_N&#41; 
 &#61; \sigma_1 \sigma_2 &#43; \sigma_2 \sigma_3 &#43; \sigma_3 \sigma_4 &#43; \ldots &#43; \sigma_N \sigma_1 \:.
@@</p>
<p>The classic &quot;transfer matrix&quot; trick for computing &#36;Z&#36; goes as follows:
@@
Z &#61; \sum_&#123;&#123;\sigma&#125;&#125; \exp \left&#40;\frac&#123;-1&#125;&#123;T&#125; \sum_n \sigma_n \sigma_&#123;n&#43;1&#125;\right&#41;
 &#61; \sum_&#123;&#123;\sigma&#125;&#125; \prod_&#123;n&#125; e^&#123;-&#40;\sigma_n \sigma_&#123;n&#43;1&#125;&#41;/ T&#125;
 &#61; \text&#123;Tr&#125; \left&#40;M^N \right&#41;
@@</p>
<p>where &#36;\text&#123;Tr&#125;&#36; means &quot;trace&quot; and the transfer matrix &#36;M&#36; is a 2x2 matrix with elements</p>
<p>@@
M_&#123;\sigma^&#123;\&#33;&#125; \sigma^\prime&#125; &#61; e^&#123;-&#40;\sigma^&#123;\&#33;&#125; \sigma^\prime&#41;/T&#125; \ .
@@</p>
<p>Pictorially, we can view &#36;\text&#123;Tr&#125;\left&#40;M^N\right&#41;&#36; as a chain of tensor contractions around a
circle:</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;60&#37;&quot; src&#61;&quot;docs/book/images/TRG_1dIsingZ.png&quot;/&gt;</p>
<p>With each 2-index tensor in the above diagram defined to equal the matrix M, it is an exact
rewriting of the partition function &#36;Z&#36; as a tensor network.</p>
<p>For this one-dimensional case, the trick to compute &#36;Z&#36; is just to diagonalize &#36;M&#36;. 
If &#36;M&#36; has eigenvalues &#36;\lambda_1&#36; and &#36;\lambda_2&#36;, it follows that 
&#36;Z &#61; \lambda_1^N &#43; \lambda_2^N&#36; by the basis invariance of the trace operation.</p>
<h3>Two dimensions</h3>
<p>Now let us consider the main problem of interest. For two dimensions, the energy function
can be written as
@@
E&#40;\sigma_1, \sigma_2, \ldots&#41; &#61; \sum_&#123;\langle i j \rangle&#125; \sigma_i \sigma_j
@@
where the notation &#36;\langle i j \rangle&#36; means the sum only includes &#36;i,j&#36; which are
neighboring sites. It helps to visualize the system:</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;60&#37;&quot; src&#61;&quot;docs/book/images/TRG_2dIsingZ.png&quot;/&gt;</p>
<p>In the figure above, the black arrows are the Ising spins &#36;\sigma&#36; and the 
blue lines represent the local energies &#36;\sigma_i \sigma_j&#36;.
The total energy &#36;E&#36; of each configuration is the sum of all of these local energies.</p>
<p>Interestingly, it is again possible to rewrite the partition function sum
&#36;Z&#36; as a network of contracted tensors. Define the tensor &#36;A^&#123;\sigma_t \sigma_r \sigma_b \sigma_l&#125;&#36;
to be 
@@
A^&#123;\sigma_t \sigma_r \sigma_b \sigma_l&#125; &#61; e^&#123;-&#40;\sigma_t \sigma_r &#43; \sigma_r \sigma_b &#43; \sigma_b \sigma_l &#43; \sigma_l \sigma_t&#41;/T&#125;
@@</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;15&#37;&quot; src&#61;&quot;docs/book/images/TRG_Atensor.png&quot;/&gt;</p>
<p>The interpretation of this tensor is that it computes the local energies between the four spins that
live on its indices, and its value is the Boltzmann probability weight &#36;e^&#123;-E/T&#125;&#36; associated with
these energies. Note its similarity to the one-dimensional transfer matrix &#36;M&#36;.</p>
<p>With &#36;A&#36; thus defined, the partition function &#36;Z&#36; for the two-dimensional Ising model can
be found by contracting the following network of &#36;A&#36; tensors:</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;35&#37;&quot; src&#61;&quot;docs/book/images/TRG_2dPeriodic.png&quot;/&gt;</p>
<p>The above drawing is of a lattice of 32 Ising spins &#40;recall that the spins live on
the tensor indices&#41;. The indices at the edges of this square wrap around in a periodic
fashion because the energy function was defined using periodic boundary conditions.</p>
<h2>The TRG Algorithm</h2>
<p>TRG is a strategy for computing the above 2d network, which is just equal to a single number &#36;Z&#36;
&#40;since there are no uncontracted external indices&#41;. The TRG approach is to locally replace 
individual &#36;A&#36; tensors with pairs of lower-rank tensors which guarantee the result of the contraction
remains the same to a good approximation. These smaller tensors can then be recombined in a different 
way that results in a more sparse, yet equivalent network.</p>
<p>Referring to the original &#36;A&#36; tensor as &#36;A_0&#36;, the first &quot;move&quot; of 
TRG is to factorize the &#36;A_0&#36; tensor in two different ways:</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;85&#37;&quot; src&#61;&quot;docs/book/images/TRG_factor2ways.png&quot;/&gt;</p>
<p>Both factorizations can be computed using the &#91;&#91;singular value decomposition &#40;SVD&#41;|book/itensor_factorizing&#93;&#93;.
For example, to compute the first factorization, view &#36;A_0&#36; as a matrix with a collective &quot;row&quot;
index &#36;\sigma_l&#36; and &#36;\sigma_t&#36; and collective &quot;column&quot; index &#36;\sigma_b&#36; and &#36;\sigma_r&#36;. 
After performing an SVD of &#36;A_0&#36; in this way, further factorize the singular value matrix &#36;S&#36; as &#36;S &#61; \sqrt&#123;S&#125; \sqrt&#123;S&#125;&#36; and 
absorb each &#36;\sqrt&#123;S&#125;&#36; factor into 
U and V to create the factors &#36;F_1&#36; and &#36;F_2&#36;. Pictorially:</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;100&#37;&quot; src&#61;&quot;docs/book/images/TRG_factorizing.png&quot;/&gt;</p>
<p>Importantly, the SVD is only done approximately by retaining just the &#36;\chi&#36; largest singular
values and discarding the columns of U and V corresponding to the smaller singular values.
This truncation is crucial for keeping the cost of the TRG algorithm under control.</p>
<p>Making the above substitutions, either
&#36;A_0&#61;F_1 F_3&#36; or &#36;A_0&#61;F_2 F_4&#36; on alternating lattice sites, transforms the
original tensor network into the following network:</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;90&#37;&quot; src&#61;&quot;docs/book/images/TRG_network1.png&quot;/&gt;</p>
<p>Finally by contracting the four F tensors in the following way</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;40&#37;&quot; src&#61;&quot;docs/book/images/TRG_group.png&quot;/&gt;</p>
<p>one obtains the tensor &#36;A_1&#36; which has four indices just like &#36;A_0&#36;.
Contracting the &#36;A_1&#36; tensors in a square-lattice pattern gives the 
same result &#40;up to SVD truncation errors&#41; as contracting the original &#36;A_0&#36; tensors,
only there are half as many &#36;A_1&#36; tensors &#40;each &#36;A_0&#36; consists
of two F&#39;s while each &#36;A_1&#36; consists of four F&#39;s&#41;.</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;80&#37;&quot; src&#61;&quot;docs/book/images/TRG_recombine.png&quot;/&gt;</p>
<p>To compute &#36;Z&#36; defined by contracting a square lattice of &#36;2^&#123;1&#43;N&#125;&#36; tensors, one
repeats the above two steps &#40;factor and recombine&#41; N times until only a single
tensor remains. Calling this final tensor &#36;A_N&#36;, the result &#36;Z&#36; of contracting
the original network is equal to the following &quot;double trace&quot; of &#36;A_N&#36;:</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;20&#37;&quot; src&#61;&quot;docs/book/images/TRG_top.png&quot;/&gt;</p>
<h3>Implementing TRG in ITensor</h3>
<p>Finally we are ready to implement the algorithm above using ITensor.
At the end of this section we will arrive at a complete working code,
but let&#39;s look at each piece step by step.</p>
<p>To get started, start with the following empty application:</p>
<pre><code>#include &quot;itensor/all_basic.h&quot;

using namespace itensor;

int main&#40;&#41; 
&#123;

//Our code will go here

return 0;
&#125;</code></pre>
<p>First define some basic parameters of the calculation, such as the temperature &quot;T&quot;; the
maximum number  of singular values &quot;maxm&quot;; and the top-most scale we want to reach
with TRG:</p>
<pre><code>Real T &#61; 3.0;
int maxm &#61; 20;
int topscale &#61; 6;</code></pre>
<p>Next define the indices which will go on the initial &quot;A&quot;
tensor:</p>
<pre><code>auto m0 &#61; 2;
auto x &#61; Index&#40;&quot;x0&quot;,m0,Xtype&#41;;
auto y &#61; Index&#40;&quot;y0&quot;,m0,Ytype&#41;;
auto x2 &#61; prime&#40;x,2&#41;;
auto y2 &#61; prime&#40;y,2&#41;;</code></pre>
<p>Here it is good practice to save the index dimension &#36;m_0&#61;2&#36; into its own variable
to prevent &quot;magic numbers&quot; from appearing later in the code. The constants
<code>XType</code> and <code>YType</code> are &quot;IndexType&quot; tags which let us conveniently manipulate only horizontal
or only vertical indices later on. It is also convenient to save copies of x and 
y with prime level raised to 2 as the variables x2 and y2.</p>
<p>Now let&#39;s create the &quot;A&quot; tensor defining the partition function and set its values as discussed
in the previous section:</p>
<pre><code>auto A &#61; ITensor&#40;x,y2,x2,y&#41;;

auto Sig &#61; &#91;&#93;&#40;int s&#41; &#123; return 1.-2.*&#40;s-1&#41;; &#125;;

auto E0 &#61; -4.0;

for&#40;auto s1 : range1&#40;m0&#41;&#41;
for&#40;auto s2 : range1&#40;m0&#41;&#41;
for&#40;auto s3 : range1&#40;m0&#41;&#41;
for&#40;auto s4 : range1&#40;m0&#41;&#41;
    &#123;
    auto E &#61; Sig&#40;s1&#41;*Sig&#40;s2&#41;&#43;Sig&#40;s2&#41;*Sig&#40;s3&#41;
            &#43;Sig&#40;s3&#41;*Sig&#40;s4&#41;&#43;Sig&#40;s4&#41;*Sig&#40;s1&#41;;
    auto P &#61; exp&#40;-&#40;E-E0&#41;/T&#41;;
    A.set&#40;x&#40;s1&#41;,y2&#40;s2&#41;,x2&#40;s3&#41;,y&#40;s4&#41;,P&#41;;
    &#125;</code></pre>
<p>The first line creates the &quot;A&quot; tensor with indices x,y2,x2,y and all elements set to zero.
The next line defines a &quot;lambda&quot; function bound to the variable name Sig which converts integers
1 and 2 into Ising spin values &#43;1.0 and -1.0. To set the elements of A, we loop over integers
s1,s2,s3,s4. The function <code>range1&#40;d&#41;</code> returns an object that can be used in a <code>for</code> loop to
iterate over the integers 1,2,3,...,d.</p>
<p>One slight difference with the convention of the previous section is that here 
the Boltzmann probability weight P has an energy shift of <code>E0 &#61; -4.0</code> in the exponent. This 
will keep the norm of the rescaled A tensors from growing too quickly later. Though it changes
&#36;Z&#36;, it does so in a way that is easy to account for.</p>
<p>Finally we are ready to dive into the main TRG algorithm loop. To reach scale &#36;N&#36; we need to
do &#36;N-1&#36; steps, so we will write a loop that does this number of steps:</p>
<pre><code>for&#40;auto scale : range&#40;topscale&#41;&#41;
    &#123;
    printfln&#40;&quot;\n---------- Scale &#37;d -&gt; &#37;d  ----------&quot;,scale,1&#43;scale&#41;;

    //...TRG algorithm code will go here...

	&#125;</code></pre>
<p>In contrast to the earlier <code>range1</code> function which starts at 1, <code>range&#40;topscale&#41;</code> makes the <code>for</code> loop
run over 0,1,...,topscale-1.</p>
<p>In the body of this loop let us first &quot;grab&quot; the x and y indices of the A tensor at the
current scale. </p>
<pre><code>auto y &#61; noprime&#40;findtype&#40;A,Ytype&#41;&#41;;
auto y2 &#61; prime&#40;y,2&#41;;
auto x &#61; noprime&#40;findtype&#40;A,Xtype&#41;&#41;;
auto x2 &#61; prime&#40;x,2&#41;;</code></pre>
<p>Although on the first pass these are just the same indices we defined before, 
new indices will arise as A refers to tensors at higher scales.</p>
<p>The function <code>findtype&#40;T,IndexType&#41;</code> searches through the indices of a tensor and returns
the first index whose type matches the specified IndexType. Since we want the version of 
this index with prime level 0, we call noprime to reset the prime level to zero. We 
also create versions of these indices with prime level 2 for convenience.</p>
<p>Now it&#39;s time to decompose the current A tensor as discussed
in the previous section. First the <code>A&#61;F1*F3</code> factorization:</p>
<pre><code>auto F1 &#61; ITensor&#40;x2,y&#41;;
auto F3 &#61; ITensor&#40;x,y2&#41;;
auto xname &#61; format&#40;&quot;x&#37;d&quot;,scale&#43;1&#41;;

factor&#40;A,F1,F3,&#123;&quot;Maxm&quot;,maxm,&quot;ShowEigs&quot;,true,
				&quot;IndexType&quot;,Xtype,&quot;IndexName&quot;,xname&#125;&#41;;</code></pre>
<p>We create the ITensors F1 and F3 with the indices of A we
want them to have after the factorization. This tells the <code>factor</code> routine how
to group the indices of A. Along with the tensors, we pass some &#91;&#91;named arguments|tutorials/args&#93;&#93;.
The argument &quot;Maxm&quot; puts a limit on how many singular values are kept in the SVD. Setting &quot;ShowEigs&quot;
to <code>true</code> shows helpful information about the truncation of singular values &#40;actually the squares
of the singular values which are called &quot;density matrix eigenvalues&quot;&#41;. Also we pass an IndexType and 
name for the new index which will be created to connect F1 and F3.
The line <code>auto xname &#61; format&#40;&quot;x&#37;d&quot;,scale&#43;1&#41;;</code> is a string formatting operation; if for example <code>scale &#61;&#61; 2</code>
then xname will be &quot;x3&quot;.</p>
<p>We can write very similar code to do the <code>A&#61;F2*F4</code> factorization, the main difference being
which indices of A we request to end up on F2 versus F4:</p>
<pre><code>auto F2 &#61; ITensor&#40;x,y&#41;;
auto F4 &#61; ITensor&#40;y2,x2&#41;;
auto yname &#61; format&#40;&quot;y&#37;d&quot;,scale&#43;1&#41;;

factor&#40;A,F2,F4,&#123;&quot;Maxm&#61;&quot;,maxm,&quot;ShowEigs&#61;&quot;,true,
				&quot;IndexType&#61;&quot;,Ytype,&quot;IndexName&#61;&quot;,yname&#125;&#41;;</code></pre>
<p>For the last step of the TRG algorithm we combine the factors of the A tensor at the current
scale to create a &quot;renormalized&quot; A tensor at the next scale:</p>
<pre><code>auto l13 &#61; commonIndex&#40;F1,F3&#41;;
A &#61; F1 * noprime&#40;F4&#41; * prime&#40;F2,2&#41; * prime&#40;F3,l13,2&#41;;</code></pre>
<p>The first line grabs a copy of the index common to F1 and F3, which is convenient to
have for the next line. The second line first contracts F1 with F4, then the result of this
contraction with F2, and finally with F3 to produce the new A tensor. The functions
wrapping the F tensors adjust the prime levels of various indices so that the indices we
want contracted with match while the indices we don&#39;t want contracted will have unique
prime levels.</p>
<p>In more detail, <code>noprime&#40;F4&#41;</code> returns a copy of F4 &#40;without copying F4&#39;s data&#41; such that all
indices have prime level 0. Calling <code>prime&#40;F2,2&#41;</code> increases the prime level of all of F2&#39;s indices
by 2. And <code>prime&#40;F3,l13,2&#41;</code> raises the prime level of just the index <code>l13</code> by 2. Try drawing
the tensor diagram showing the contraction of the F tensors to convince yourself that the 
prime levels work out correctly.</p>
<p>Last but not least, after we have proceeded through each scale
we want to take the last A tensor at the &quot;top scale&quot; specified and 
compute observables from it. Though this tensor contains a wealth of information,
we will look at the simplest case of computing the partition function &#36;Z&#36;.</p>
<p>To obtain &#36;Z&#36; from the top tensor, all we have to do is trace both the x indices with each
other and trace the y indices with each other, which results in a scalar tensor whose
value is &#36;Z&#36;:</p>
<p>&lt;img class&#61;&quot;diagram&quot; width&#61;&quot;20&#37;&quot; src&#61;&quot;docs/book/images/TRG_top.png&quot;/&gt;</p>
<p>In ITensor, you can compute a trace by creating a special type of sparse ITensor 
called a <code>delta</code>. A <code>delta</code> tensor has only diagonal elements, all equal to 1.0.
Pictorially, you can view the delta tensors as the dashed lines in the above diagram.</p>
<p>Let us grab the x and y indices of the top tensor:</p>
<pre><code>auto xt &#61; noprime&#40;findtype&#40;A,Xtype&#41;&#41;;
auto yt &#61; noprime&#40;findtype&#40;A,Ytype&#41;&#41;;
auto xt2 &#61; prime&#40;xt,2&#41;;
auto yt2 &#61; prime&#40;yt,2&#41;;</code></pre>
<p>Then use these indices to create delta tensors:</p>
<pre><code>auto Trx &#61; delta&#40;xt,xt2&#41;;
auto Try &#61; delta&#40;yt,yt2&#41;;</code></pre>
<p>Finally we contract these tensors with &quot;A&quot; and convert the result to a real
number to obtain &#36;Z&#36;:</p>
<pre><code>auto Z &#61; &#40;Trx*A*Try&#41;.real&#40;&#41;;</code></pre>
<p>An interesting quantity to print out is &#36;\ln&#40;Z&#41;/N_s&#36; where &#36;N_s &#61; 2^&#123;1&#43;N&#125;&#36; 
is the number of sites &quot;contained&quot; in the top tensor at scale &#36;N&#36;:</p>
<pre><code>Real Ns &#61; pow&#40;2,1&#43;topscale&#41;;
printfln&#40;&quot;log&#40;Z&#41;/Ns &#61; &#37;.12f&quot;,log&#40;Z&#41;/Ns&#41;;</code></pre>
<p>With the conventions for the probability weights we have chosen, we can check
&#36;\ln&#40;Z&#41;/N_s&#36; against the following exact result &#40;for an infinite-sized system&#41;:
@@
\ln&#40;Z&#41;/N_s &#61; -2\beta &#43; \frac&#123;1&#125;&#123;2&#125; \ln&#40;2&#41; &#43; \frac&#123;1&#125;&#123;2\pi&#125; \int_0^\pi\, d\theta \ln&#123;\Big&#91; \cosh&#40;2\beta&#41;^2 &#43; \frac&#123;1&#125;&#123;k&#125; \sqrt&#123;1&#43;k^2-2k\cos&#40;2\theta&#41;&#125;\,\Big&#93;&#125;
@@
where the constant &#36;k&#61;1/\sinh&#40;2\beta&#41;^2&#36; and recall &#36;\beta&#61;1/T&#36;.</p>
<p>Click the link just below to view a complete, working sample code you can compile yourself. Compare the value of
&#36;\ln&#40;Z&#41;/N_s&#36; you get to the exact result. How does adjusting <code>maxm</code> and <code>topscale</code> affect your result?</p>
<p>&lt;div class&#61;&quot;example_clicker&quot;&gt;Click here to view the full example code&lt;/div&gt;</p>
<pre><code>include:docs/book/trg.cc</code></pre>
<p>&lt;img class&#61;&quot;icon&quot; src&#61;&quot;docs/install.png&quot;/&gt;&amp;nbsp;&lt;a href&#61;&quot;docs/book/trg.cc&quot;&gt;Download the full example code&lt;/a&gt;</p>
<h3>Next Steps for You to Try</h3>
<ol>
<li><p>Modify the sample application to read in parameters</p>
</li>
</ol>
<p>from a file, using the ITensor &#91;&#91;input parameter system|tutorials/input&#93;&#93;.</p>
<ol start="2">
<li><p>Following the details in the appendix of the &quot;Tensor Network Renormalization&quot; paper arxiv:1412.0732, for the critical temperature &#36;T_c&#61;2/\ln&#40;1&#43;\sqrt&#123;2&#125;&#41;&#36; trace  the top-scale &quot;A&quot; tensor in the x direction, then diagonalize the resulting matrix to obtain the leading scaling dimensions of the critical 2 dimensional Ising model.</p>
</li>
<li><p>Following the paper arxiv:0903.1069, include an &quot;impurity tensor&quot; which measures the magnetization of a single Ising spin, and compare your results at various temperatures to the <a href="https://en.wikipedia.org/wiki/Square-lattice_Ising_model">exact solution</a>.</p>
</li>
</ol>
<p><em>Pro Tip</em>: for tasks 2 and 3 above, it is a good idea to modify the TRG code such that A gets 
normalized after each step, for example by adding a line <code>A /&#61; norm&#40;A&#41;;</code>. 
The exact normalization is not so important &#40;trace norm versus Frobenius norm&#41;; the idea is to 
prevent A from getting too big, which will definitely occur after too many iterations.
When computing observables such as the magnetization, it is sufficient to use the &quot;effective&quot; 
partition function &#36;Z_\text&#123;eff&#125;&#36; obtained by double-tracing the top-scale A, regardless
of how it is normalized.</p>
<h3>References</h3>
<ul>
<li><p><em>The original paper on TRG</em>:</p>
<p>Levin and Nave, &quot;Tensor Renormalization Group Approach to Two-Dimensional Classical Lattice Models&quot;, <a href="http://dx.doi.org/10.1103/PhysRevLett.99.120601">PRL 99, 120601</a> &#40;2007&#41;  cond-mat/0611687</p>
</li>
<li><p><em>Paper on TRG with very useful figures &#40;particularly Fig. 5&#41;</em>:</p>
<p>Gu, Levin, and Wen,  &quot;Tensor-entanglement renormalization group approach as a unified method for symmetry breaking and topological phase transitions&quot; <a href="http://dx.doi.org/10.1103/PhysRevB.78.205116">PRB 78, 205116</a> &#40;2008&#41;  arxiv:0806.3509</p>
</li>
<li><p><em>TNR is an extension of TRG which qualitatively improves TRG&#39;s fixed-point behavior  and can be used to generate MERA tensor networks</em>:</p>
<p>Evenbly and Vidal, &quot;Tensor Network Renormalization&quot; <a href="http://dx.doi.org/10.1103/PhysRevB.80.155131">PRL 115, 180405</a> &#40;2015&#41; arxiv:1412.0732</p>
</li>
</ul>
<p>&lt;br/&gt;</p>
<p>&lt;span style&#61;&quot;float:left;&quot;&gt;&lt;img src&#61;&quot;docs/arrowleft.png&quot; class&#61;&quot;icon&quot;&gt;
&#91;&#91;Factorizing ITensors|book/itensor_factorizing&#93;&#93;
&lt;/span&gt;
&lt;span style&#61;&quot;float:right;&quot;&gt;&lt;img src&#61;&quot;docs/arrowright.png&quot; class&#61;&quot;icon&quot;&gt;
&#91;&#91;IQTensor Overview|book/iqtensor_overview&#93;&#93;
&lt;/span&gt;</p>
<p>&lt;br/&gt;</p>

</div> <!--class=full edit docs-->

<div id="footer"></div>


</div> <!--class="main"-->

<script type="text/javascript">
$(document).ready(function() 
    {
    $('.rounded').corner("7px");

    $('.example_clicker').next().hide();

    $('.example_clicker').click(
    function(e)
        {
        /*
        $(e.target).toggle(
        function(e) { $(e.target).text("Show Example").stop(); },
        function(e) { $(e.target).text("Hide Example").stop(); });
        */

        $(e.target).next('.highlight').slideToggle(100);

        /* Change text */
        var str = $(e.target).text();
        if(str.search("Show") > -1)
            {
            str = str.split("Show").join("Hide");
            }
        else
            {
            str = str.split("Hide").join("Show");
            }

        var set_text = function() { $(e.target).text(str); }
        window.setTimeout(set_text, 200);
        });
    });
</script>

</body>
</html>
